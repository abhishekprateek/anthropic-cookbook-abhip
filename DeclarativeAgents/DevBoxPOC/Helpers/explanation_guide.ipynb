{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric Definitions\n",
    "We'll evaluate our system based on 5 key metrics: Precision, Recall, F1 Score, Mean Reciprocal Rank (MRR), and End-to-End Accuracy.\n",
    "\n",
    "## Retrieval Metrics:\n",
    "\n",
    "### Precision\n",
    "Precision represents the proportion of retrieved chunks that are actually relevant. It answers the question: \"Of the chunks we retrieved, how many were correct?\"\n",
    "\n",
    "Key points:\n",
    "- High precision indicates an efficient system with few false positives.\n",
    "- Low precision suggests many irrelevant chunks are being retrieved.\n",
    "- Our system retrieves a minimum of 3 chunks per query, which may affect precision scores.\n",
    "\n",
    "Formula:\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{True Positives}}{\\text{Total Retrieved}} = \\frac{|\\text{Retrieved} \\cap \\text{Correct}|}{|\\text{Retrieved}|}\n",
    "$$\n",
    "\n",
    "### Recall\n",
    "Recall measures the completeness of our retrieval system. It answers the question: \"Of all the correct chunks that exist, how many did we manage to retrieve?\"\n",
    "\n",
    "Key points:\n",
    "- High recall indicates comprehensive coverage of necessary information.\n",
    "- Low recall suggests important chunks are being missed.\n",
    "- Recall is crucial for ensuring the LLM has access to all needed information.\n",
    "\n",
    "Formula:\n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{True Positives}}{\\text{Total Correct}} = \\frac{|\\text{Retrieved} \\cap \\text{Correct}|}{|\\text{Correct}|}\n",
    "$$\n",
    "\n",
    "### F1 Score\n",
    "The F1 score provides a balanced measure between precision and recall. It's particularly useful when you need a single metric to evaluate system performance, especially with uneven class distributions.\n",
    "\n",
    "Key points:\n",
    "- F1 score ranges from 0 to 1, with 1 representing perfect precision and recall.\n",
    "- It's the harmonic mean of precision and recall, tending towards the lower of the two values.\n",
    "- Useful in scenarios where both false positives and false negatives are important.\n",
    "\n",
    "Formula:\n",
    "$$\n",
    "\\text{F1 Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "Interpreting F1 score:\n",
    "- An F1 score of 1.0 indicates perfect precision and recall.\n",
    "- An F1 score of 0.0 indicates the worst performance.\n",
    "- Generally, the higher the F1 score, the better the overall performance.\n",
    "\n",
    "### Balancing Precision, Recall, and F1 Score:\n",
    "- There's often a trade-off between precision and recall.\n",
    "- Our system's minimum chunk retrieval favors recall over precision.\n",
    "- The optimal balance depends on the specific use case.\n",
    "- In many RAG systems, high recall is often prioritized, as LLMs can filter out less relevant information during generation.\n",
    "\n",
    "### Mean Reciprocal Rank (MRR) @k\n",
    "MRR measures how well our system ranks relevant information. It helps us understand how quickly a user would find what they're looking for if they started from the top of our retrieved results.\n",
    "\n",
    "Key points:\n",
    "- MRR ranges from 0 to 1, where 1 is perfect (correct answer always first).\n",
    "- It only considers the rank of the first correct result for each query.\n",
    "- Higher MRR indicates better ranking of relevant information.\n",
    "\n",
    "Formula:\n",
    "$$\n",
    "\\text{MRR} = \\frac{1}{|Q|} \\sum_{i=1}^{|Q|} \\frac{1}{\\text{rank}_i}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- |Q| is the total number of queries\n",
    "- rank_i is the position of the first relevant item for the i-th query\n",
    "\n",
    "## End to End Metrics:\n",
    "\n",
    "### End to End Accuracy\n",
    "We use an LLM-as-judge (Claude 3.5 Sonnet) to evaluate whether the generated answer is correct based on the question and ground truth answer.\n",
    "\n",
    "Formula:\n",
    "$$\n",
    "\\text{End to End Accuracy} = \\frac{\\text{Number of Correct Answers}}{\\text{Total Number of Questions}}\n",
    "$$\n",
    "\n",
    "This metric evaluates the entire pipeline, from retrieval to answer generation."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
